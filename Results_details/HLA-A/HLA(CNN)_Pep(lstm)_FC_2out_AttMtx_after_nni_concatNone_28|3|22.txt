Run the classic model (with concat=None), with an extension of matrix W that suppose to illustrate Attention (explained in model details).
It is after NNI on the model parameters. The results are separated according HLA freq.

Script:
    HLA_Pep_model_2outputs.py

Hyper-parameters:
    All hyper-parameters outside the model structure (epoch, batch size, optimizer, loss, early stopping, train-val-test split)
    and optimizer parameters (lr, weight_decay, alpha)  are like in 'HLA(CNN)_Pep(lstm)_FC_2out_after_nni_4concat_6|3|22.txt'

    model params are different (because the model is different)
    hidden size: 64
    activation function: tanh
    att_layers: 9

Model details:
    HLA_Pep_AttMatrix
    Pep and HLA sequences are embedded in their AEs (CNN, lstm) like before, but after that, they were multiplied with
    matrix W that illustrates the attention act. (find the 'connection' between each part in the pep seq and the hla seq)
    This 'attention matrix' could has some 'attention heads' - so that is the hyper-parameters 'att_layers', mean: the dim-3 of the matrix
    Afterward, we take the diagonal of the result matrix and insert to FC, and there are 2 outputs, like before.

Data file:
    'mhc_pep00A_NoArtificialNeg_Pep7_11_2labels.csv'
    same processing like before

Results:
    Epoch: 120 / 300
    loss train:  0.0047  |  auc train:  0.9687  |  r2 train:  0.5917
    loss val:  0.0051    |  auc val:  0.9555    |  r2 val:  0.5517
    Case high:	auc test:  0.9552  |  r2 test:  0.5393
    Case medium:	auc test:  0.9726  |  r2 test:  0.7783
    Case low:	auc test:  0.8315  |  r2 test:  0.4908

Path to results:
    'Autoencoder/HLA_Pep_CNN_LSTM_FC_Results/CNN_LSTM_FC_version10_NoneAttMtx'