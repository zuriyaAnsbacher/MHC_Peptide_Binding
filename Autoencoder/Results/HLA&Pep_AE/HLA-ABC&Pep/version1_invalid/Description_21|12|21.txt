Script:
    HLA_Pep_model_binary.py

Hyper-parameters:
    batch size: 50
    epochs: 250
    optimizer: Adam. parameters: only fc layers after the AEs
    loss function: F.binary_cross_entropy_with_logits
    counter of early stopping: 30
    train size: 68% (0.8 * 0.85)
    val size: 12% (0.8 * 0.15)
    test size: 20%

Model details:
    Pep sequences were inserted to trained model: AE_lstm.
        (Its details are in 'AE_lstm_Pep_2|12|21.txt'.
        Model was loaded from: 'Pep_LSTM_Results/LSTM_last_output_1_layer_and_fc_lr=0.001_without_regularization/lstm_model_encoding_dim=20.pt')
    HLA sequences were inserted to trained model: AE_CNN.
        (Its details are in 'AE_CNN_HLA_8|12|21.txt'.
        Model was loaded from: 'HLA-A_CNN_Results/CNN_version2/cnn_embedding=15_encoding=50.pt')

    Then, the encoded sequences of HLA and pep were concatenated and insert to FC layers:
        self.fc1 = nn.Sequential(
                nn.Linear(self.encoding_dim_pep + self.encoding_dim_hla, 32),
                nn.ReLU(),
                nn.Dropout(0.1),
            )
            self.fc2 = nn.Linear(32, 1)
            *** no sigmoid because loss function already contains ***

Data file:
    mhc_pep05_30perc_neg_PepLen9.csv
    (Original file: mhc_ligand_full_05.csv)
    Processing done by:
        - filtered by: Linear peptide, Homo sapiens (in Epitope column, not Host)
        - peptides: remove artificial addition ('OVYLVMEL + OX(6M)' -> 'OVYLVMEL')
        - only peptides with len = 9
        - labels conversion:
            Positive, Positive-High, Positive-Intermediate -> 1
            Negative, Positive-Low  -> 0
        - create negative samples randomly. ratio of ~ 66% positive, 33% negative
    Finally, contains 107106 pairs of pep-hla and their label

Results:
    Num epochs: 250 / 250
    Test auc: 0.9188


# Note 1: 'bce_loss.pdf' in results path is not good because of a mistake that was in the code
# Note 2: There is a mistake I found later: The CNN AE were trained on HLA-A sequences only, but the combined model was trained on A,B,C

